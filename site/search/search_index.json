{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawling There is 2 web crawling types in this website : Web Content Mining Web Structure Mining Feel free to use and modify the codes in this site ! Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom.","title":"Home"},{"location":"#web-crawling","text":"There is 2 web crawling types in this website : Web Content Mining Web Structure Mining Feel free to use and modify the codes in this site ! Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom.","title":"Web Crawling"},{"location":"Clustering/","text":"Clustering Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics. In this program , I used one of the clustering method called K-Means. K-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. After the clustering process has done , we have to find the silhouette value to see the cluster is good or not. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Here is the code for performing clustering with k-means then calculate the value of silhouette : from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score kmeans = KMeans(n_clusters=3, random_state=15).fit(tfidf_matrix.todense()) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix.todense(), kmeans.labels_, random_state=0) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Code above will do the clustering with 3 clusters. Feel free to customize how many clusters do you want. Documents that has been clustered will writen in Kluster_label.csv file. After this the program will calculate the value of silhouette using the library of scikit-learn. The more closer your value to 1 , the better the cluster was. The program also printing all the documents and its cluster.","title":"Clustering"},{"location":"Clustering/#clustering","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics. In this program , I used one of the clustering method called K-Means. K-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. After the clustering process has done , we have to find the silhouette value to see the cluster is good or not. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. Here is the code for performing clustering with k-means then calculate the value of silhouette : from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score kmeans = KMeans(n_clusters=3, random_state=15).fit(tfidf_matrix.todense()) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix.todense(), kmeans.labels_, random_state=0) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) Code above will do the clustering with 3 clusters. Feel free to customize how many clusters do you want. Documents that has been clustered will writen in Kluster_label.csv file. After this the program will calculate the value of silhouette using the library of scikit-learn. The more closer your value to 1 , the better the cluster was. The program also printing all the documents and its cluster.","title":"Clustering"},{"location":"Complete Codes/","text":"Complete Codes Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Here are the complete code from the beginning in case you missed something in each step : # For crawling purpose import requests from bs4 import BeautifulSoup # For Graoh purpose import networkx as nx import matplotlib.pyplot as plt def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges root = \"https://jagokata.com/\" nodelist = [root] done = [root] edgelist = [] from datetime import datetime start_time = datetime.now() deep1 = getAllLinks(root) #nodelist = add_nodes_from_list(nodelist, deep1) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: # add edge each time accessing new link edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) # deep 2 deep2 = getAllLinks(link) #print(c, link, len(deep2)); c+=1 for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) g = nx.Graph() g = g.to_directed() # Masukin ke Graph g.add_edges_from(edgelist) # deklarasi pos (koordinat) (otomatis) pos = nx.spring_layout(g) # hitung pagerank pr = nx.pagerank(g) # Membuat Label print(\"keterangan node:\") label= {} jum = 0 for i in range(len(nodelist)): label[nodelist[i]]=i print(i, nodelist[i], pr[nodelist[i]]) jum = i print(\"Jumlah keseluruhan link\", jum) for x in range(len(data)): for y in range(len(data)): if data[x][1] > data[y][1]: data[x],data[y] = data[y],data[x] #Misah link dalam = [] luar = [] for x in nodelist: if 'https://jagokata.com/' in x: dalam.append(x) else: luar.append(x) print('Link Dalam') for i in dalam: print(i) print('Link Luar') for j in luar: print(j) # Draw Graph plt.figure(1) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_size=8) end_time = datetime.now() print('Waktu = {}'.format(end_time - start_time)) # show figure plt.axis(\"off\") plt.show() I added a few line of codes that count the running time of the program. So you can see how much time needed to run this program with your own website link to crawl. Feel free to modify this program that meets your need.","title":"Complete Codes"},{"location":"Complete Codes/#complete-codes","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Here are the complete code from the beginning in case you missed something in each step : # For crawling purpose import requests from bs4 import BeautifulSoup # For Graoh purpose import networkx as nx import matplotlib.pyplot as plt def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges root = \"https://jagokata.com/\" nodelist = [root] done = [root] edgelist = [] from datetime import datetime start_time = datetime.now() deep1 = getAllLinks(root) #nodelist = add_nodes_from_list(nodelist, deep1) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: # add edge each time accessing new link edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) # deep 2 deep2 = getAllLinks(link) #print(c, link, len(deep2)); c+=1 for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) g = nx.Graph() g = g.to_directed() # Masukin ke Graph g.add_edges_from(edgelist) # deklarasi pos (koordinat) (otomatis) pos = nx.spring_layout(g) # hitung pagerank pr = nx.pagerank(g) # Membuat Label print(\"keterangan node:\") label= {} jum = 0 for i in range(len(nodelist)): label[nodelist[i]]=i print(i, nodelist[i], pr[nodelist[i]]) jum = i print(\"Jumlah keseluruhan link\", jum) for x in range(len(data)): for y in range(len(data)): if data[x][1] > data[y][1]: data[x],data[y] = data[y],data[x] #Misah link dalam = [] luar = [] for x in nodelist: if 'https://jagokata.com/' in x: dalam.append(x) else: luar.append(x) print('Link Dalam') for i in dalam: print(i) print('Link Luar') for j in luar: print(j) # Draw Graph plt.figure(1) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_size=8) end_time = datetime.now() print('Waktu = {}'.format(end_time - start_time)) # show figure plt.axis(\"off\") plt.show() I added a few line of codes that count the running time of the program. So you can see how much time needed to run this program with your own website link to crawl. Feel free to modify this program that meets your need.","title":"Complete Codes"},{"location":"Conclusion/","text":"Conclusion Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Here are the complete code from the beginning in case you missed something in each step : import requests import urllib.request as link import re from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score def write_csv(nama_file, isi, tipe='w'): with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) def crawl(src): global c try : for i in range(1,6): src = link.request.urlopen(\"https://jagokata.com/kata-bijak/kata-pepatah.html?page=\"+str(i)) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') kata = soup.findAll(class_='fbquote') penutur = soup.findAll(class_='auteurfbnaam') for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) VALUES (\"%s\", \"%s\")' %(a, b)); choice = input(\"Tampilkan data? Y/T \").upper() if choice == \"Y\": cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) except ValueError: print('Download selesai') def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() stop = stopword.remove(txt) Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() stem = stemmer.stem(stop) return stem def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].append(0) VSM[-1].append(d.get(i)) conn = sqlite3.connect('dbs.db') c = 1 #src = link.urlopen(\"https://jagokata.com/kata-bijak/kata-pepatah.html?page=\"+str(page)).read() #src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" choice = input(\"Perbarui data? Y/T \").upper() if choice == \"Y\": conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') for i in range (1,6): crawl(str(i)) conn.commit() print(\"Please Wait. Building VSM...\") cursor = conn.execute(\"SELECT * from Kata\") cursor = cursor.fetchall() pertama = True corpus = list() c=1 for row in cursor: #print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) #VSM[-1].append(row[2]) #VSM[-1].append(row[3]) with open('tableview.csv', mode='w') as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in VSM: tbl_writer.writerow(row) write_csv(\"bow_manual.csv\", VSM) # BoW using library vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) BoW_matrix = vectorizer.fit_transform(corpus) write_csv(\"bow_lib.csv\", BoW_matrix.toarray()) # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') # Clustering kmeans = KMeans(n_clusters=3, random_state=15).fit(tfidf_matrix.todense()) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix.todense(), kmeans.labels_, random_state=0) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) This program only do 1 method of clustering , so the result was not good enough. It needs few more experiment and adding a few methods of clustering to makes it perfect. I have a lack experience of coding skill in Python , so I use Python libraries so often.","title":"Conclusion"},{"location":"Conclusion/#conclusion","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. Here are the complete code from the beginning in case you missed something in each step : import requests import urllib.request as link import re from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score def write_csv(nama_file, isi, tipe='w'): with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) def crawl(src): global c try : for i in range(1,6): src = link.request.urlopen(\"https://jagokata.com/kata-bijak/kata-pepatah.html?page=\"+str(i)) page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') kata = soup.findAll(class_='fbquote') penutur = soup.findAll(class_='auteurfbnaam') for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) VALUES (\"%s\", \"%s\")' %(a, b)); choice = input(\"Tampilkan data? Y/T \").upper() if choice == \"Y\": cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) except ValueError: print('Download selesai') def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() stop = stopword.remove(txt) Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() stem = stemmer.stem(stop) return stem def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].append(0) VSM[-1].append(d.get(i)) conn = sqlite3.connect('dbs.db') c = 1 #src = link.urlopen(\"https://jagokata.com/kata-bijak/kata-pepatah.html?page=\"+str(page)).read() #src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" choice = input(\"Perbarui data? Y/T \").upper() if choice == \"Y\": conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') for i in range (1,6): crawl(str(i)) conn.commit() print(\"Please Wait. Building VSM...\") cursor = conn.execute(\"SELECT * from Kata\") cursor = cursor.fetchall() pertama = True corpus = list() c=1 for row in cursor: #print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) #VSM[-1].append(row[2]) #VSM[-1].append(row[3]) with open('tableview.csv', mode='w') as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in VSM: tbl_writer.writerow(row) write_csv(\"bow_manual.csv\", VSM) # BoW using library vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) BoW_matrix = vectorizer.fit_transform(corpus) write_csv(\"bow_lib.csv\", BoW_matrix.toarray()) # calculating TF-IDF vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() #print(tfidf_matrix) write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') # Clustering kmeans = KMeans(n_clusters=3, random_state=15).fit(tfidf_matrix.todense()) write_csv(\"Kluster_label.csv\", [kmeans.labels_]) s_avg = silhouette_score(tfidf_matrix.todense(), kmeans.labels_, random_state=0) print(s_avg) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) This program only do 1 method of clustering , so the result was not good enough. It needs few more experiment and adding a few methods of clustering to makes it perfect. I have a lack experience of coding skill in Python , so I use Python libraries so often.","title":"Conclusion"},{"location":"Getting Started/","text":"Getting Started Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This program will run in Python 3.7 version First, we need to import some libraries called beautifulsoup4 and requests*. So you need to install these libraries if you haven't install it to your computer. If you have followed the web content mining tutorial before, you dont need to install this library again. Install the libraries that will be used in this program Before you type the code below, make sure your computer is connected to the Internet . Open your command window and type : pip install beautifulsoup4 This code will download the beautifulsoap4 library and install it automatically in your computer if its meet the requirement. If the code above failed , try this magical code : pip install bs4 Don't close your command window yet, type this one : pip install requests Same like before, this code will download requests library and install it automatically in your computer if its meet the requirement. You have 2 more libraries to install : pip install networkx After above installation have complete, install the last library pip install matplotlib Program List After libraries installation, make sure your computer is still connected to the Internet or this program may not run. Run your Python 3.7 and follow this First of all we need to import the libraries we already installed import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt requests library will be used to make requests to the Internet to download any page you want to crawl BeautifulSoup library will be used to convert the page you have downloaded to BeautifulSoup object networkx library will be used to count the pagerank and draw it to matplotlib graph matplotlib library will be used to show the graph of your mined web structure","title":"Getting Started"},{"location":"Getting Started/#getting-started","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This program will run in Python 3.7 version First, we need to import some libraries called beautifulsoup4 and requests*. So you need to install these libraries if you haven't install it to your computer. If you have followed the web content mining tutorial before, you dont need to install this library again. Install the libraries that will be used in this program Before you type the code below, make sure your computer is connected to the Internet . Open your command window and type : pip install beautifulsoup4 This code will download the beautifulsoap4 library and install it automatically in your computer if its meet the requirement. If the code above failed , try this magical code : pip install bs4 Don't close your command window yet, type this one : pip install requests Same like before, this code will download requests library and install it automatically in your computer if its meet the requirement. You have 2 more libraries to install : pip install networkx After above installation have complete, install the last library pip install matplotlib Program List After libraries installation, make sure your computer is still connected to the Internet or this program may not run. Run your Python 3.7 and follow this First of all we need to import the libraries we already installed import requests from bs4 import BeautifulSoup import networkx as nx import matplotlib.pyplot as plt requests library will be used to make requests to the Internet to download any page you want to crawl BeautifulSoup library will be used to convert the page you have downloaded to BeautifulSoup object networkx library will be used to count the pagerank and draw it to matplotlib graph matplotlib library will be used to show the graph of your mined web structure","title":"Getting Started"},{"location":"Graph/","text":"Graph Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. A Graph is a non-linear data structure consisting of nodes and edges. The nodes are sometimes also referred to as vertices and the edges are lines or arcs that connect any two nodes in the graph. More formally a Graph can be defined as, A Graph consists of a finite set of vertices(or nodes) and set of Edges which connect a pair of nodes. In the above Graph, the set of vertices V = {0,1,2,3,4} and the set of edges E = {01, 12, 23, 34, 04, 14, 13}. Graphs are used to solve many real-life problems. Graphs are used to represent networks. The networks may include paths in a city or telephone network or circuit network. Graphs are also used in social networks like linkedIn, Facebook. For example, in Facebook, each person is represented with a vertex(or node). Each node is a structure and contains information like person id, name, gender, locale etc.","title":"Graph"},{"location":"Graph/#graph","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. A Graph is a non-linear data structure consisting of nodes and edges. The nodes are sometimes also referred to as vertices and the edges are lines or arcs that connect any two nodes in the graph. More formally a Graph can be defined as, A Graph consists of a finite set of vertices(or nodes) and set of Edges which connect a pair of nodes. In the above Graph, the set of vertices V = {0,1,2,3,4} and the set of edges E = {01, 12, 23, 34, 04, 14, 13}. Graphs are used to solve many real-life problems. Graphs are used to represent networks. The networks may include paths in a city or telephone network or circuit network. Graphs are also used in social networks like linkedIn, Facebook. For example, in Facebook, each person is represented with a vertex(or node). Each node is a structure and contains information like person id, name, gender, locale etc.","title":"Graph"},{"location":"GraphCode/","text":"Graph Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This code will print a graph with networkx and matplotlib library and use the variables from the step before. Using the edgelist list from the code before to add the edges to the graph. import networkx as nx import matplotlib.pyplot as plt g = nx.Graph() g = g.to_directed() g.add_edges_from(edgelist) pos = nx.spring_layout(g) plt.figure(1) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_size=8) plt.axis(\"off\") plt.show() As the directed graph created, you will see your links nodes and edges.","title":"Graph Code"},{"location":"GraphCode/#graph","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This code will print a graph with networkx and matplotlib library and use the variables from the step before. Using the edgelist list from the code before to add the edges to the graph. import networkx as nx import matplotlib.pyplot as plt g = nx.Graph() g = g.to_directed() g.add_edges_from(edgelist) pos = nx.spring_layout(g) plt.figure(1) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_size=8) plt.axis(\"off\") plt.show() As the directed graph created, you will see your links nodes and edges.","title":"Graph"},{"location":"Page Rank Code/","text":"Page Rank Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This code will count the page rank of every links related to your website and print it. Using an empty list to write the labels of link numbers, link, and link page ranks from nodelist list from the code before. import networkx as nx pr = nx.pagerank(g) print(\"keterangan node:\") label= {} jum = 0 for i in range(len(nodelist)): label[nodelist[i]]=i print(i, nodelist[i], pr[nodelist[i]]) jum = i print(\"Jumlah keseluruhan link\", jum) Be patient, my website have about 3600+ links to be printed.","title":"Page Rank Code"},{"location":"Page Rank Code/#page-rank","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This code will count the page rank of every links related to your website and print it. Using an empty list to write the labels of link numbers, link, and link page ranks from nodelist list from the code before. import networkx as nx pr = nx.pagerank(g) print(\"keterangan node:\") label= {} jum = 0 for i in range(len(nodelist)): label[nodelist[i]]=i print(i, nodelist[i], pr[nodelist[i]]) jum = i print(\"Jumlah keseluruhan link\", jum) Be patient, my website have about 3600+ links to be printed.","title":"Page Rank"},{"location":"Page Rank/","text":"Page Rank Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web , with the purpose of \"measuring\" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is referred to as the PageRank of E . A PageRank results from a mathematical algorithm based on the webgraph , created by all World Wide Web pages as nodes and hyperlinks as edges, taking into consideration authority hubs such as cnn.com or usa.gov . The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (\" incoming links \"). A page that is linked to by many pages with high PageRank receives a high rank itself. Numerous academic papers concerning PageRank have been published since Page and Brin's original paper.In practice, the PageRank concept may be vulnerable to manipulation. Research has been conducted into identifying falsely influenced PageRank rankings. The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank. Other link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com ),the IBM CLEVER project , the TrustRank algorithm and the Hummingbird algorithm. Algorithm The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called \"iterations\", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value. A probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a \"50% chance\" of something happening. Hence, a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to the document with the 0.5 PageRank. Simplified algorithm Assume a small universe of four web pages: A , B , C and D . Links from a page to itself are ignored. Multiple outbound links from one page to another page are treated as a single link. PageRank is initialized to the same value for all pages. In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1. However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. Hence the initial value for each page in this example is 0.25. The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links. If the only links in the system were from pages B , C , and D to A , each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75. Suppose instead that page B had a link to pages C and A , page C had a link to page A , and page D had links to all three pages. Thus, upon the first iteration, page B would transfer half of its existing value, or 0.125, to page A and the other half, or 0.125, to page C . Page C would transfer all of its existing value, 0.25, to the only page it links to, A . Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A . At the completion of this iteration, page A will have a PageRank of approximately 0.458. In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ) . In the general case, the PageRank value for any page u can be expressed as: i.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set Bu (the set containing all pages linking to page u ), divided by the number L ( v ) of links from page v .","title":"Page Rank"},{"location":"Page Rank/#page-rank","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, such as the World Wide Web , with the purpose of \"measuring\" its relative importance within the set. The algorithm may be applied to any collection of entities with reciprocal quotations and references. The numerical weight that it assigns to any given element E is referred to as the PageRank of E . A PageRank results from a mathematical algorithm based on the webgraph , created by all World Wide Web pages as nodes and hyperlinks as edges, taking into consideration authority hubs such as cnn.com or usa.gov . The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it (\" incoming links \"). A page that is linked to by many pages with high PageRank receives a high rank itself. Numerous academic papers concerning PageRank have been published since Page and Brin's original paper.In practice, the PageRank concept may be vulnerable to manipulation. Research has been conducted into identifying falsely influenced PageRank rankings. The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank. Other link-based ranking algorithms for Web pages include the HITS algorithm invented by Jon Kleinberg (used by Teoma and now Ask.com ),the IBM CLEVER project , the TrustRank algorithm and the Hummingbird algorithm.","title":"Page Rank"},{"location":"Page Rank/#algorithm","text":"The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called \"iterations\", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value. A probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a \"50% chance\" of something happening. Hence, a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to the document with the 0.5 PageRank.","title":"Algorithm"},{"location":"Page Rank/#simplified-algorithm","text":"Assume a small universe of four web pages: A , B , C and D . Links from a page to itself are ignored. Multiple outbound links from one page to another page are treated as a single link. PageRank is initialized to the same value for all pages. In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1. However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. Hence the initial value for each page in this example is 0.25. The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links. If the only links in the system were from pages B , C , and D to A , each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75. Suppose instead that page B had a link to pages C and A , page C had a link to page A , and page D had links to all three pages. Thus, upon the first iteration, page B would transfer half of its existing value, or 0.125, to page A and the other half, or 0.125, to page C . Page C would transfer all of its existing value, 0.25, to the only page it links to, A . Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A . At the completion of this iteration, page A will have a PageRank of approximately 0.458. In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ) . In the general case, the PageRank value for any page u can be expressed as: i.e. the PageRank value for a page u is dependent on the PageRank values for each page v contained in the set Bu (the set containing all pages linking to page u ), divided by the number L ( v ) of links from page v .","title":"Simplified algorithm"},{"location":"Start Mining/","text":"Start Mining Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. We need to create the functions to getting all links and drawing the nodges and edges def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges The functions above will crawl all of tag and get the 'http' tag in a website and draw all the nodes and edges for every another link the website has related to. After this we have to define the website we will crawl the structure and call the function above root = \"https://jagokata.com/\" nodelist = [root] done = [root] edgelist = [] deep1 = getAllLinks(root) #nodelist = add_nodes_from_list(nodelist, deep1) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: # add edge each time accessing new link edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) # deep 2 deep2 = getAllLinks(link) #print(c, link, len(deep2)); c+=1 for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) This code will crawl the structure in the depth of 3. Feel free to customize the code above if you want another depth. This code also count and print the root count and also print every dead links in the website","title":"Start Mining"},{"location":"Start Mining/#start-mining","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. We need to create the functions to getting all links and drawing the nodges and edges def getAllLinks(src): try: ind = src.find(':')+3 url = src[ind:] page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') tags = soup.findAll(\"a\") links = [] for tag in tags: try: link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: print(\"Error 404 : Page \"+src+\" not found\") return list() def add_nodes_from_list(old_nodes, new_nodes): for node in new_nodes: if not node in old_nodes: old_nodes.append(node) return old_nodes def add_edges_from_list(old_edges, from_, to_list): for to_ in to_list: edge = (from_, to) if not edge in old_edges: old_edges.append(edge) return old_edges The functions above will crawl all of tag and get the 'http' tag in a website and draw all the nodes and edges for every another link the website has related to. After this we have to define the website we will crawl the structure and call the function above root = \"https://jagokata.com/\" nodelist = [root] done = [root] edgelist = [] deep1 = getAllLinks(root) #nodelist = add_nodes_from_list(nodelist, deep1) print(\"banyak root : \", len(deep1)) c=1 for link in deep1: # add edge each time accessing new link edge = (root, link) if not edge in edgelist: edgelist.append(edge) if not link in done: nodelist.append(link) done.append(link) # deep 2 deep2 = getAllLinks(link) #print(c, link, len(deep2)); c+=1 for link2 in deep2: edge = (link, link2) if not edge in edgelist: edgelist.append(edge) if not link2 in done: nodelist.append(link2) done.append(link2) deep3 = getAllLinks(link2) for link3 in deep3: edge = (link2, link3) if not edge in edgelist: edgelist.append(edge) if not link3 in nodelist: nodelist.append(link3) This code will crawl the structure in the depth of 3. Feel free to customize the code above if you want another depth. This code also count and print the root count and also print every dead links in the website","title":"Start Mining"},{"location":"Text Extraction/","text":"Text Extraction Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. In this occasion , we will use 2 methods for the Text Extraction : Stopword Remove Removing words that is not important and still searched by the search engine. Stemming Converting all words to the basic words But before you do the tasks above , make sure you have installed Sastrawi library from Python. Make sure your computer is connected to the internet and then open your command prompt then type : pip install sastrawi Here is the code : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() stop = stopword.remove(txt) Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() stem = stemmer.stem(stop) return stem Sastrawi library will be used for Stopword Removal tasks when you are crawling website that use Indonesian Language The code above will remove all the words that is not important and every words that is considered important will be converted into the basic words. We have to count the important words above right now. For example we have 2 documents : \"Saya berbicara bahasa Indonesia ketika sedang berbicara dengan Ibu\" \"Ibu saya tidak bisa bahasa selain Indonesia\" The code below will make a VSM building table like this : Document Bicara Bahasa Indonesia Ketika Sedang Dengan Ibu Tidak Bisa Selain 1 2 1 1 1 1 1 1 0 0 0 2 0 1 1 0 0 0 1 1 1 1 def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].append(0) VSM[-1].append(d.get(i)) The countWord function above will count the word appear in each document , while the add_row_vsm function will create VSM matrix of the word above. cursor = conn.execute(\"SELECT * from Kata\") cursor = cursor.fetchall() pertama = True corpus = list() c=1 for row in cursor: #print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) #VSM[-1].append(row[2]) #VSM[-1].append(row[3]) After this we have to write it to csv file to see the result : import csv def write_csv(nama_file, isi, tipe='w'): with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) with open('tableview.csv', mode='w') as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in VSM: tbl_writer.writerow(row) write_csv(\"bow_manual.csv\", VSM) The code above will generate a csv file that include your words that have been selected by VSM and write the frequency of each word appear in every document TF - IDF Tf-idf stands for term frequency-inverse document frequency , and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. TF: Term Frequency , which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). IDF: Inverse Document Frequency , which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Install scikit-learn library using command prompt : pip install scikit-learn Here is the code , I use sklearn library to make it more easier : from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') The code above will generate a csv file that write your word tf-idf value in each document.","title":"Text Extraction"},{"location":"Text Extraction/#text-extraction","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. In this occasion , we will use 2 methods for the Text Extraction : Stopword Remove Removing words that is not important and still searched by the search engine. Stemming Converting all words to the basic words But before you do the tasks above , make sure you have installed Sastrawi library from Python. Make sure your computer is connected to the internet and then open your command prompt then type : pip install sastrawi Here is the code : from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory def preprosesing(txt): SWfactory = StopWordRemoverFactory() stopword = SWfactory.create_stop_word_remover() stop = stopword.remove(txt) Sfactory = StemmerFactory() stemmer = Sfactory.create_stemmer() stem = stemmer.stem(stop) return stem Sastrawi library will be used for Stopword Removal tasks when you are crawling website that use Indonesian Language The code above will remove all the words that is not important and every words that is considered important will be converted into the basic words. We have to count the important words above right now. For example we have 2 documents : \"Saya berbicara bahasa Indonesia ketika sedang berbicara dengan Ibu\" \"Ibu saya tidak bisa bahasa selain Indonesia\" The code below will make a VSM building table like this : Document Bicara Bahasa Indonesia Ketika Sedang Dengan Ibu Tidak Bisa Selain 1 2 1 1 1 1 1 1 0 0 0 2 0 1 1 0 0 0 1 1 1 1 def countWord(txt): d = dict() for i in txt.split(): if d.get(i) == None: d[i] = txt.count(i) return d def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].append(0) VSM[-1].append(d.get(i)) The countWord function above will count the word appear in each document , while the add_row_vsm function will create VSM matrix of the word above. cursor = conn.execute(\"SELECT * from Kata\") cursor = cursor.fetchall() pertama = True corpus = list() c=1 for row in cursor: #print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 txt = row[0] cleaned = preprosesing(txt) corpus.append(cleaned) d = countWord(cleaned) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d) #VSM[-1].append(row[2]) #VSM[-1].append(row[3]) After this we have to write it to csv file to see the result : import csv def write_csv(nama_file, isi, tipe='w'): with open(nama_file, mode=tipe) as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in isi: tbl_writer.writerow(row) with open('tableview.csv', mode='w') as tbl: tbl_writer = csv.writer(tbl, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) for row in VSM: tbl_writer.writerow(row) write_csv(\"bow_manual.csv\", VSM) The code above will generate a csv file that include your words that have been selected by VSM and write the frequency of each word appear in every document TF - IDF Tf-idf stands for term frequency-inverse document frequency , and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. TF: Term Frequency , which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document). IDF: Inverse Document Frequency , which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: IDF(t) = log_e(Total number of documents / Number of documents with term t in it). Install scikit-learn library using command prompt : pip install scikit-learn Here is the code , I use sklearn library to make it more easier : from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() tfidf_matrix = vectorizer.fit_transform(corpus) feature_name = vectorizer.get_feature_names() write_csv(\"tfidf.csv\", [feature_name]) write_csv(\"tfidf.csv\", tfidf_matrix.toarray(), 'a') The code above will generate a csv file that write your word tf-idf value in each document.","title":"Text Extraction"},{"location":"Web Structure Mining/","text":"Web Structure Mining Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. The goal of Web structure mining is to generate structural summary about the Web site and Web page. Technically, Web content mining mainly focuses on the structure of inner-document, while Web structure mining tries to discover the link structure of the hyperlinks at the inter-document level. Based on the topology of the hyperlinks, Web structure mining will categorize the Web pages and generate the information, such as the similarity and relationship between different Web sites. Web structure mining can also have another direction -- discovering the structure of Web document itself. This type of structure mining can be used to reveal the structure (schema) of Web pages, this would be good for navigation purpose and make it possible to compare/integrate Web page schemes. This type of structure mining will facilitate introducing database techniques for accessing information in Web pages by providing a reference schema. The detailed works on it can be referred to [ Madria 1999 ]. In general, if a Web page is linked to another Web page directly, or the Web pages are neighbors, we would like to discover the relationships among those Web pages. The relations maybe fall in one of the types, such as they related by synonyms or ontology, they may have similar contents, both of them may sit in the same Web server therefore created by the same person. Another task of Web structure mining is to discover the nature of the hierarchy or network of hyperlink in the Web sites of a particular domain. This may help to generalize the flow of information in Web sites that may represent some particular domain, therefore the query processing will be easier and more efficient. Web structure mining has a nature relation with the Web content mining , since it is very likely that the Web documents contain links, and they both use the real or primary data on the Web. It's quite often to combine these two mining tasks in an application.","title":"Web Structure Mining"},{"location":"Web Structure Mining/#web-structure-mining","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. The goal of Web structure mining is to generate structural summary about the Web site and Web page. Technically, Web content mining mainly focuses on the structure of inner-document, while Web structure mining tries to discover the link structure of the hyperlinks at the inter-document level. Based on the topology of the hyperlinks, Web structure mining will categorize the Web pages and generate the information, such as the similarity and relationship between different Web sites. Web structure mining can also have another direction -- discovering the structure of Web document itself. This type of structure mining can be used to reveal the structure (schema) of Web pages, this would be good for navigation purpose and make it possible to compare/integrate Web page schemes. This type of structure mining will facilitate introducing database techniques for accessing information in Web pages by providing a reference schema. The detailed works on it can be referred to [ Madria 1999 ]. In general, if a Web page is linked to another Web page directly, or the Web pages are neighbors, we would like to discover the relationships among those Web pages. The relations maybe fall in one of the types, such as they related by synonyms or ontology, they may have similar contents, both of them may sit in the same Web server therefore created by the same person. Another task of Web structure mining is to discover the nature of the hierarchy or network of hyperlink in the Web sites of a particular domain. This may help to generalize the flow of information in Web sites that may represent some particular domain, therefore the query processing will be easier and more efficient. Web structure mining has a nature relation with the Web content mining , since it is very likely that the Web documents contain links, and they both use the real or primary data on the Web. It's quite often to combine these two mining tasks in an application.","title":"Web Structure Mining"},{"location":"WebCrawling/","text":"Web Crawling Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This program will run in Python 3.7 version First, we need to import some libraries called beautifulsoup4 , requests , and sqlite3 . So you need to install these libraries if you haven't install it to your computer. Install the libraries that will be used in this program Before you type the code below, make sure your computer is connected to the Internet . Open your command window and type : pip install beautifulsoup4 This code will download the beautifulsoap4 library and install it automatically in your computer if its meet the requirement. If the code above failed , try this magical code : pip install bs4 Don't close your command window yet, type this one : pip install requests Same like before, this code will download requests library and install it automatically in your computer if its meet the requirement. What about the sqlite3 library ? You don't have to install it manually, because this library will installed once you install Python. Program List After libraries installation, make sure your computer is still connected to the Internet or this program may not run. Run your Python 3.7 and follow this First of all we need to import the libraries we already installed import requests from bs4 import BeautifulSoup import sqlite3 requests library will be used to make requests to the Internet to download any page you want to crawl BeautifulSoup library will be used to convert the page you have downloaded to BeautifulSoup object sqlite3 library will be used for database connection and operation After this we need to make database file and creating tables to save our crawled data from the page you want conn = sqlite3.connect('dbs.db') conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') We will make a new variable conn for connecting to the database by creating database file called dbs . Then this program will delete table named Kata if its exist. After that we create a table called Kata with 2 fields : Katas and Penutur both using text type data. Then we need to download a code website html and convert it to BeautifulSoup object src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') In this program, I use the above link for example. You can change to any website you want Then we have to search the class of the data we want to crawl The quotes is located in fbquotes class of an paragraph, so we have to catch items in that class kata = soup.findAll(class_='fbquote') After this we have to extract every data that we catch and save it to the database we already created above : for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) \\ VALUES (\"%s\", \"%s\")' %(a, b)); Using for iteration, we extract all data from kata variable and save it to the new a variable and we insert it to the table Kata we created. Where is the data located ? How can I see it ? cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) Finally, we have to select all data from Kata table above and print it to the screen using for iteration. Here are the complete code : import requests from bs4 import BeautifulSoup import sqlite3 conn = sqlite3.connect('dbs.db') conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') kata = soup.findAll(class_='fbquote') penutur = soup.findAll(class_='auteurfbnaam') for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) \\ VALUES (\"%s\", \"%s\")' %(a, b)); cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) Weaknesses In this tutorial, we create the database and table with 2 fields manually . So if you want to create another databases with more table fields, just modify the code below to anything you want : conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') This tutorial will help you to crawl only 1 page . You have to crawl it manually if you want to crawl another pages.","title":"Crawling"},{"location":"WebCrawling/#web-crawling","text":"Muhammad Miftakhus Sobari 160411100045 Penambangan dan Pencarian Web Teknik Informatika - Universitas Trunojoyo Madura Pengampu : Mulaab, S.Si., M.Kom. This program will run in Python 3.7 version First, we need to import some libraries called beautifulsoup4 , requests , and sqlite3 . So you need to install these libraries if you haven't install it to your computer. Install the libraries that will be used in this program Before you type the code below, make sure your computer is connected to the Internet . Open your command window and type : pip install beautifulsoup4 This code will download the beautifulsoap4 library and install it automatically in your computer if its meet the requirement. If the code above failed , try this magical code : pip install bs4 Don't close your command window yet, type this one : pip install requests Same like before, this code will download requests library and install it automatically in your computer if its meet the requirement. What about the sqlite3 library ? You don't have to install it manually, because this library will installed once you install Python. Program List After libraries installation, make sure your computer is still connected to the Internet or this program may not run. Run your Python 3.7 and follow this First of all we need to import the libraries we already installed import requests from bs4 import BeautifulSoup import sqlite3 requests library will be used to make requests to the Internet to download any page you want to crawl BeautifulSoup library will be used to convert the page you have downloaded to BeautifulSoup object sqlite3 library will be used for database connection and operation After this we need to make database file and creating tables to save our crawled data from the page you want conn = sqlite3.connect('dbs.db') conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') We will make a new variable conn for connecting to the database by creating database file called dbs . Then this program will delete table named Kata if its exist. After that we create a table called Kata with 2 fields : Katas and Penutur both using text type data. Then we need to download a code website html and convert it to BeautifulSoup object src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') In this program, I use the above link for example. You can change to any website you want Then we have to search the class of the data we want to crawl The quotes is located in fbquotes class of an paragraph, so we have to catch items in that class kata = soup.findAll(class_='fbquote') After this we have to extract every data that we catch and save it to the database we already created above : for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) \\ VALUES (\"%s\", \"%s\")' %(a, b)); Using for iteration, we extract all data from kata variable and save it to the new a variable and we insert it to the table Kata we created. Where is the data located ? How can I see it ? cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) Finally, we have to select all data from Kata table above and print it to the screen using for iteration. Here are the complete code : import requests from bs4 import BeautifulSoup import sqlite3 conn = sqlite3.connect('dbs.db') conn.execute('drop table if exists Kata') conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') src = \"https://jagokata.com/kata-bijak/kata-pepatah.html\" page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') kata = soup.findAll(class_='fbquote') penutur = soup.findAll(class_='auteurfbnaam') for i in range(len(kata)): a = kata[i].getText() b = penutur[i].getText() conn.execute('INSERT INTO Kata(Katas, Penutur) \\ VALUES (\"%s\", \"%s\")' %(a, b)); cursor = conn.execute(\"SELECT * from Kata\") for row in cursor: print(row) Weaknesses In this tutorial, we create the database and table with 2 fields manually . So if you want to create another databases with more table fields, just modify the code below to anything you want : conn.execute('''CREATE TABLE Kata (Katas TEXT NOT NULL, Penutur TEXT NOT NULL);''') This tutorial will help you to crawl only 1 page . You have to crawl it manually if you want to crawl another pages.","title":"Web Crawling"}]}